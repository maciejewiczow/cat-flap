{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73713bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projekty\\misc\\cat-flap\\.venv\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from random import randint\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "156c0d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (224, 224)\n",
    "batch_size = 32\n",
    "validation_split = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c77315e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = randint(0, 2**32 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "466a3eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = 'datasets/processed_binary'\n",
    "baseWeightsSavePath = 'weights/best_binary.keras'\n",
    "baseFinetunedSavePath = 'weights/best_finetuned_binary.keras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ff4bf61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18042f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.MobileNetV2(\n",
    "    input_shape=IMG_SIZE + (3,), include_top=False, weights=\"imagenet\"\n",
    ")\n",
    "# Freeze the base model initially\n",
    "base_model.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2c12389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1766 files belonging to 2 classes.\n",
      "Using 1413 files for training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1766 files belonging to 2 classes.\n",
      "Using 353 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_ds = keras.utils.image_dataset_from_directory(\n",
    "    dataDir,\n",
    "    validation_split=validation_split,\n",
    "    subset=\"training\",\n",
    "    seed=seed,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=batch_size,\n",
    "    label_mode=\"binary\",\n",
    ")\n",
    "\n",
    "val_ds = keras.utils.image_dataset_from_directory(\n",
    "    dataDir,\n",
    "    validation_split=validation_split,\n",
    "    subset=\"validation\",\n",
    "    seed=seed,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=batch_size,\n",
    "    label_mode=\"binary\",\n",
    ")\n",
    "\n",
    "# Create data augmentation layer\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomRotation(0.15, fill_mode=\"nearest\"),\n",
    "        layers.RandomTranslation(0.2, 0.2, fill_mode=\"nearest\"),\n",
    "        layers.RandomZoom(0.1, fill_mode=\"nearest\"),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomBrightness(0.3),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Apply augmentation only to training dataset\n",
    "def augment_images(image, label):\n",
    "    return data_augmentation(image, training=True), label\n",
    "\n",
    "\n",
    "# Normalize pixel values to [0,1]\n",
    "normalization_layer = layers.Rescaling(1.0 / 255)\n",
    "\n",
    "# Process training dataset (with augmentation)\n",
    "train_ds = (\n",
    "    train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "    .map(augment_images, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# Process validation dataset (without augmentation)\n",
    "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y)).prefetch(\n",
    "    buffer_size=tf.data.AUTOTUNE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f6b39c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback_base = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=baseWeightsSavePath,\n",
    "    monitor=\"val_accuracy\",  # You can use 'val_loss' for less overfitting\n",
    "    save_best_only=True,  # Critical: only save if validation improves\n",
    "    save_weights_only=False,  # Save entire model (architecture + weights)\n",
    "    mode=\"max\",  # 'max' for accuracy, 'min' for loss\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "checkpoint_callback_finetune = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=baseFinetunedSavePath,\n",
    "    monitor=\"val_accuracy\",  # You can use 'val_loss' for less overfitting\n",
    "    save_best_only=True,  # Critical: only save if validation improves\n",
    "    save_weights_only=False,  # Save entire model (architecture + weights)\n",
    "    mode=\"max\",  # 'max' for accuracy, 'min' for loss\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Callback 2: Early stopping to prevent overfitting\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=10,  # Stop after 5 epochs without improvement\n",
    "    restore_best_weights=True,  # Keep the best weights when stopping\n",
    ")\n",
    "\n",
    "# Callback 3: Reduce learning rate when plateauing\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.5,  # Reduce learning rate by half\n",
    "    patience=3,  # Wait 3 epochs\n",
    "    min_lr=1e-7,  # Minimum learning rate\n",
    ")\n",
    "\n",
    "callbacks = [early_stop, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e2380bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        base_model,\n",
    "        keras.layers.GlobalAveragePooling2D(),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(128, activation=\"relu\"),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de635a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9df6dad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 506ms/step - accuracy: 0.5095 - loss: 0.8163\n",
      "Epoch 1: val_accuracy improved from None to 0.57507, saving model to weights/best_binary.keras\n",
      "\n",
      "Epoch 1: finished saving model to weights/best_binary.keras\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 674ms/step - accuracy: 0.5244 - loss: 0.7640 - val_accuracy: 0.5751 - val_loss: 0.6688 - learning_rate: 0.0010\n",
      "Epoch 2/15\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 515ms/step - accuracy: 0.4999 - loss: 0.7169\n",
      "Epoch 2: val_accuracy improved from 0.57507 to 0.64873, saving model to weights/best_binary.keras\n",
      "\n",
      "Epoch 2: finished saving model to weights/best_binary.keras\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 661ms/step - accuracy: 0.5209 - loss: 0.7044 - val_accuracy: 0.6487 - val_loss: 0.6354 - learning_rate: 0.0010\n",
      "Epoch 3/15\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534ms/step - accuracy: 0.5342 - loss: 0.6955\n",
      "Epoch 3: val_accuracy did not improve from 0.64873\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 661ms/step - accuracy: 0.5513 - loss: 0.6896 - val_accuracy: 0.6317 - val_loss: 0.6633 - learning_rate: 0.0010\n",
      "Epoch 4/15\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 498ms/step - accuracy: 0.5513 - loss: 0.6929\n",
      "Epoch 4: val_accuracy did not improve from 0.64873\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 627ms/step - accuracy: 0.5690 - loss: 0.6874 - val_accuracy: 0.6459 - val_loss: 0.6536 - learning_rate: 0.0010\n",
      "Epoch 5/15\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520ms/step - accuracy: 0.5518 - loss: 0.6878\n",
      "Epoch 5: val_accuracy did not improve from 0.64873\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 645ms/step - accuracy: 0.5563 - loss: 0.6896 - val_accuracy: 0.5892 - val_loss: 0.7291 - learning_rate: 0.0010\n",
      "Epoch 6/15\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 490ms/step - accuracy: 0.5339 - loss: 0.7006\n",
      "Epoch 6: val_accuracy did not improve from 0.64873\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 621ms/step - accuracy: 0.5556 - loss: 0.6892 - val_accuracy: 0.5836 - val_loss: 0.7088 - learning_rate: 5.0000e-04\n",
      "Epoch 7/15\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 506ms/step - accuracy: 0.5563 - loss: 0.6858\n",
      "Epoch 7: val_accuracy did not improve from 0.64873\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 628ms/step - accuracy: 0.5619 - loss: 0.6836 - val_accuracy: 0.5892 - val_loss: 0.6777 - learning_rate: 5.0000e-04\n",
      "Epoch 8/15\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 502ms/step - accuracy: 0.5692 - loss: 0.6855\n",
      "Epoch 8: val_accuracy did not improve from 0.64873\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 626ms/step - accuracy: 0.5768 - loss: 0.6824 - val_accuracy: 0.6261 - val_loss: 0.6441 - learning_rate: 5.0000e-04\n",
      "Epoch 9/15\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 491ms/step - accuracy: 0.5562 - loss: 0.6854\n",
      "Epoch 9: val_accuracy did not improve from 0.64873\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 615ms/step - accuracy: 0.5683 - loss: 0.6819 - val_accuracy: 0.6487 - val_loss: 0.6286 - learning_rate: 2.5000e-04\n",
      "Epoch 10/15\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 490ms/step - accuracy: 0.5522 - loss: 0.6849\n",
      "Epoch 10: val_accuracy did not improve from 0.64873\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 615ms/step - accuracy: 0.5648 - loss: 0.6817 - val_accuracy: 0.6459 - val_loss: 0.6291 - learning_rate: 2.5000e-04\n",
      "Epoch 11/15\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 513ms/step - accuracy: 0.5690 - loss: 0.6857\n",
      "Epoch 11: val_accuracy did not improve from 0.64873\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 662ms/step - accuracy: 0.5676 - loss: 0.6851 - val_accuracy: 0.6289 - val_loss: 0.6423 - learning_rate: 2.5000e-04\n",
      "Epoch 12/15\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535ms/step - accuracy: 0.5632 - loss: 0.6811\n",
      "Epoch 12: val_accuracy did not improve from 0.64873\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 664ms/step - accuracy: 0.5754 - loss: 0.6809 - val_accuracy: 0.6487 - val_loss: 0.6306 - learning_rate: 2.5000e-04\n",
      "Epoch 13/15\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542ms/step - accuracy: 0.5615 - loss: 0.6852\n",
      "Epoch 13: val_accuracy did not improve from 0.64873\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 682ms/step - accuracy: 0.5704 - loss: 0.6810 - val_accuracy: 0.6431 - val_loss: 0.6328 - learning_rate: 1.2500e-04\n",
      "Epoch 14/15\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 531ms/step - accuracy: 0.5720 - loss: 0.6786\n",
      "Epoch 14: val_accuracy did not improve from 0.64873\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 672ms/step - accuracy: 0.5711 - loss: 0.6804 - val_accuracy: 0.6431 - val_loss: 0.6271 - learning_rate: 1.2500e-04\n",
      "Epoch 15/15\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 557ms/step - accuracy: 0.5701 - loss: 0.6798\n",
      "Epoch 15: val_accuracy improved from 0.64873 to 0.65156, saving model to weights/best_binary.keras\n",
      "\n",
      "Epoch 15: finished saving model to weights/best_binary.keras\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 697ms/step - accuracy: 0.5789 - loss: 0.6763 - val_accuracy: 0.6516 - val_loss: 0.6252 - learning_rate: 1.2500e-04\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=15,  # Initial phase\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[checkpoint_callback_base, *callbacks],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b05cbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(baseWeightsSavePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cb88959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 459ms/step - accuracy: 0.6516 - loss: 0.6252\n",
      "Validation Accuracy: 65.16%\n"
     ]
    }
   ],
   "source": [
    "validation_loss, validation_accuracy = model.evaluate(val_ds)\n",
    "print(f\"Validation Accuracy: {validation_accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85a4f12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 2: Fine-tuning last 40 layers...\n",
      "Epoch 15/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732ms/step - accuracy: 0.5776 - loss: 0.6851\n",
      "Epoch 15: val_accuracy improved from None to 0.65722, saving model to weights/best_finetuned_binary.keras\n",
      "\n",
      "Epoch 15: finished saving model to weights/best_finetuned_binary.keras\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 914ms/step - accuracy: 0.5761 - loss: 0.6839 - val_accuracy: 0.6572 - val_loss: 0.6280 - learning_rate: 1.0000e-05\n",
      "Epoch 16/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735ms/step - accuracy: 0.5619 - loss: 0.6879\n",
      "Epoch 16: val_accuracy did not improve from 0.65722\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 876ms/step - accuracy: 0.5669 - loss: 0.6850 - val_accuracy: 0.6544 - val_loss: 0.6265 - learning_rate: 1.0000e-05\n",
      "Epoch 17/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772ms/step - accuracy: 0.5357 - loss: 0.6925\n",
      "Epoch 17: val_accuracy did not improve from 0.65722\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 916ms/step - accuracy: 0.5499 - loss: 0.6899 - val_accuracy: 0.6459 - val_loss: 0.6269 - learning_rate: 1.0000e-05\n",
      "Epoch 18/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 736ms/step - accuracy: 0.5520 - loss: 0.6846\n",
      "Epoch 18: val_accuracy did not improve from 0.65722\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 874ms/step - accuracy: 0.5662 - loss: 0.6779 - val_accuracy: 0.6544 - val_loss: 0.6265 - learning_rate: 5.0000e-06\n",
      "Epoch 19/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743ms/step - accuracy: 0.5692 - loss: 0.6806\n",
      "Epoch 19: val_accuracy did not improve from 0.65722\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 882ms/step - accuracy: 0.5789 - loss: 0.6786 - val_accuracy: 0.6516 - val_loss: 0.6279 - learning_rate: 5.0000e-06\n",
      "Epoch 20/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762ms/step - accuracy: 0.5761 - loss: 0.6765\n",
      "Epoch 20: val_accuracy did not improve from 0.65722\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 897ms/step - accuracy: 0.5754 - loss: 0.6762 - val_accuracy: 0.6516 - val_loss: 0.6282 - learning_rate: 5.0000e-06\n"
     ]
    }
   ],
   "source": [
    "base_model.trainable = True\n",
    "\n",
    "fine_tune = 40\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "    layer.trainable = i >= len(base_model.layers) - fine_tune\n",
    "\n",
    "# Recompile with lower learning rate\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-5),  # Much smaller LR # pyright: ignore[reportArgumentType]\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "print(f\"\\nPhase 2: Fine-tuning last {fine_tune} layers...\")\n",
    "history_fine = model.fit(\n",
    "    train_ds,\n",
    "    initial_epoch=history.epoch[-1],\n",
    "    epochs=20,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[checkpoint_callback_finetune, *callbacks],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11d01e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(baseFinetunedSavePath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f122618f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1766 files belonging to 2 classes.\n",
      "Using 353 files for validation.\n"
     ]
    }
   ],
   "source": [
    "labels = keras.utils.image_dataset_from_directory(\n",
    "    dataDir,\n",
    "    validation_split=validation_split,\n",
    "    subset=\"validation\",\n",
    "    seed=seed,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    label_mode=\"binary\",\n",
    ").class_names # pyright: ignore[reportAttributeAccessIssue]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6b104e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 517ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 520ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 584ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 549ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 527ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 533ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 584ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 565ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 557ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 584ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = []  # store predicted labels\n",
    "y_true = []  # store true labels\n",
    "\n",
    "# iterate over the dataset\n",
    "for image_batch, label_batch in val_ds:\n",
    "    y_true.append(label_batch.numpy())\n",
    "\n",
    "    pred = model.predict(image_batch)\n",
    "\n",
    "    indices = np.argmax(pred, axis=-1)\n",
    "\n",
    "    res = np.zeros_like(pred)\n",
    "\n",
    "    np.put_along_axis(res, indices[:, None], 1, axis=-1)\n",
    "\n",
    "    y_pred.append(res)\n",
    "\n",
    "# convert the true and predicted labels into tensors\n",
    "correct_labels = np.concat(y_true, axis=0)\n",
    "predicted_labels = np.concat(y_pred, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46c55f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b823f4e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "220c2184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       close       0.00      0.00      0.00       148\n",
      "        open       0.58      1.00      0.73       205\n",
      "\n",
      "    accuracy                           0.58       353\n",
      "   macro avg       0.29      0.50      0.37       353\n",
      "weighted avg       0.34      0.58      0.43       353\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projekty\\misc\\cat-flap\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "d:\\Projekty\\misc\\cat-flap\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "d:\\Projekty\\misc\\cat-flap\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x1cad6949010>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(classification_report(correct_labels, predicted_labels,target_names=labels))\n",
    "\n",
    "cmat = confusion_matrix(\n",
    "    correct_labels,\n",
    "    predicted_labels,\n",
    "    labels=[0, 1],\n",
    ")\n",
    "\n",
    "ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cmat,\n",
    "    display_labels=labels\n",
    ").plot()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cat-flap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
